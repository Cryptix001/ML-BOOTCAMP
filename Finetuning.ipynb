{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuYEd0puIzcn3HLQB7wBA6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cryptix001/ML-BOOTCAMP/blob/main/Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e7YiLRZ4eprP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Download the dataset\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "# Step 2: Unzip the dataset\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# Step 3: Define base, train, and validation directories\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Set up paths for cats and dogs in training and validation directories\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjasVaJCfCAt",
        "outputId": "22d6007f-0e93-42c5-ff85-75e59ea2d871"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-23 00:46:58--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.202.207, 173.194.203.207, 74.125.199.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.202.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   133MB/s    in 0.5s    \n",
            "\n",
            "2024-11-23 00:46:58 (133 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Define paths to the cats and dogs directories\n",
        "cats_dir = os.path.join(train_dir, \"cats\")\n",
        "dogs_dir = os.path.join(train_dir, \"dogs\")\n",
        "\n",
        "# Initialize lists to store training data and labels\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "# Set the number of examples (you can define this as needed)\n",
        "NUMBER_OF_EXAMPLES = 1000  # Set this to the desired number of examples\n",
        "\n",
        "# Image dimensions\n",
        "img_height, img_width = 224, 224\n",
        "\n",
        "# Loop through the specified number of images\n",
        "i = 0\n",
        "while i < NUMBER_OF_EXAMPLES:\n",
        "    try:\n",
        "        if i % 2 == 0:  # Load cat images\n",
        "            im_path = os.path.join(cats_dir, os.listdir(cats_dir)[i])\n",
        "            im = Image.open(im_path).convert(\"RGB\")\n",
        "            im_resized = im.resize((img_width, img_height))\n",
        "            x_train.append(np.array(im_resized))\n",
        "            y_train.append(1)  # Label for cat\n",
        "        else:  # Load dog images\n",
        "            im_path = os.path.join(dogs_dir, os.listdir(dogs_dir)[i])\n",
        "            im = Image.open(im_path).convert(\"RGB\")\n",
        "            im_resized = im.resize((img_width, img_height))\n",
        "            x_train.append(np.array(im_resized))\n",
        "            y_train.append(0)  # Label for dog\n",
        "\n",
        "        i += 1\n",
        "    except IndexError:\n",
        "        print(f\"Not enough images to reach {NUMBER_OF_EXAMPLES}. Ending early.\")\n",
        "        break\n",
        "\n",
        "# Convert lists to numpy arrays for model training\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "print(\"Data processing complete.\")\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkjiX8P8fPIW",
        "outputId": "53e3f066-4889-4ee2-c39e-5155e5e2e09b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data processing complete.\n",
            "x_train shape: (1000, 224, 224, 3)\n",
            "y_train shape: (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ResNet50 model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers for binary classification\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)  # Sigmoid for binary classification\n",
        "\n",
        "# Final model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veOr8QxifTH4",
        "outputId": "eee5e019-2fcb-447f-ddec-9280f4891227"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to tensor format for TensorFlow compatibility\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "y_val = tf.convert_to_tensor(y_val)"
      ],
      "metadata": {
        "id": "TdLYZNAVfbWC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7807Sw_AfkIo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "pretrained_model = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    input_shape=(150, 150, 3),\n",
        "    pooling='avg',\n",
        "    classes=2,\n",
        "    weights='imagenet'\n",
        ")"
      ],
      "metadata": {
        "id": "PdK4xyQcfk_y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(x_val, y_val)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--r1oTZofqqp",
        "outputId": "c2ae19f5-77a2-4223-eca1-71c6ba0a6acd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 8s/step - accuracy: 0.7492 - loss: 0.8835 - val_accuracy: 0.9350 - val_loss: 0.1766\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 8s/step - accuracy: 0.9794 - loss: 0.0554 - val_accuracy: 0.9600 - val_loss: 0.1028\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 8s/step - accuracy: 0.9982 - loss: 0.0119 - val_accuracy: 0.9550 - val_loss: 0.1198\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 8s/step - accuracy: 1.0000 - loss: 0.0047 - val_accuracy: 0.9650 - val_loss: 0.1350\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 8s/step - accuracy: 1.0000 - loss: 8.3698e-04 - val_accuracy: 0.9600 - val_loss: 0.1396\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 8s/step - accuracy: 1.0000 - loss: 5.3964e-04 - val_accuracy: 0.9600 - val_loss: 0.1495\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 8s/step - accuracy: 1.0000 - loss: 2.6639e-04 - val_accuracy: 0.9600 - val_loss: 0.1546\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 8s/step - accuracy: 1.0000 - loss: 2.0558e-04 - val_accuracy: 0.9600 - val_loss: 0.1615\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 8s/step - accuracy: 1.0000 - loss: 1.0725e-04 - val_accuracy: 0.9600 - val_loss: 0.1649\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 10s/step - accuracy: 1.0000 - loss: 9.7674e-05 - val_accuracy: 0.9600 - val_loss: 0.1694\n"
          ]
        }
      ]
    }
  ]
}